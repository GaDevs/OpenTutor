const DOCS = [{"id":"README.md","title":"README","html":"<h1>OpenTutor Docs</h1><p>This folder contains the product and developer documentation for OpenTutor.</p><h2>Documents</h2><ul><li><code>INSTALL_CONFIG_GUIDE.md</code>: detailed installation and configuration guide with official links and setup/run steps</li><li><code>QUICKSTART.md</code>: installation and first run (Windows + Linux)</li><li><code>ARCHITECTURE.md</code>: module boundaries, data flow, and design decisions</li><li><code>PEDAGOGY.md</code>: tutor rules, FSM, correction policies, and teaching principles</li><li><code>COMMANDS.md</code>: WhatsApp commands and examples</li><li><code>TROUBLESHOOTING.md</code>: common issues and fixes</li><li><code>SECURITY.md</code>: security, privacy, and rate limiting notes</li><li><code>LICENSE_NOTES.md</code>: third-party and runtime dependency licensing notes</li></ul><h2>HTML docs viewer</h2><p>Open <code>docs-html/index.html</code> directly in your browser.</p><p>The HTML docs viewer ships with the documentation content embedded in <code>docs-html/app.js</code>, so it does not need <code>fetch</code> or a local web server.</p>"},{"id":"INSTALL_CONFIG_GUIDE.md","title":"INSTALL_CONFIG_GUIDE","html":"<h1>Installation and Configuration Guide</h1><p>This guide lists, with official links, everything you need to install and configure so OpenTutor works with text and voice on WhatsApp.</p><h2>What you need to install</h2><p>Required:</p><ul><li>Git (to clone the repository)</li><li>Node.js 20+ (project runtime)</li><li>pnpm (recommended package manager)</li><li>Python 3.10+ (STT service)</li><li>ffmpeg (STT audio decoding + WhatsApp audio conversion)</li><li>Ollama (local LLM)</li><li>Piper (local TTS)</li><li>A Piper voice model (<code>.onnx</code> + <code>.onnx.json</code>)</li></ul><p>Optional but recommended:</p><ul><li>Store <code>ffmpeg</code> inside <code>./tools/ffmpeg/</code> (auto-detected by the project)</li></ul><h2>1. Git</h2><p>Official download:</p><ul><li>https://git-scm.com/downloads</li></ul><p>Verify:</p><pre><code>git --version\n</code></pre><h2>2. Node.js (20+)</h2><p>Official download:</p><ul><li>https://nodejs.org/en/download/</li></ul><p>Recommendation:</p><ul><li>Use an LTS version (<code>20+</code>)</li></ul><p>Verify:</p><pre><code>node -v\nnpm -v\n</code></pre><h2>3. pnpm (recommended)</h2><p>Official links:</p><ul><li>Installation: https://pnpm.io/installation</li><li>Documentation: https://pnpm.io/</li></ul><p>Simple install via npm:</p><pre><code>npm install -g pnpm\n</code></pre><p>Verify:</p><pre><code>pnpm -v\n</code></pre><h2>4. Python 3.10+ (for STT)</h2><p>Official download:</p><ul><li>https://www.python.org/downloads/</li></ul><p>Verify:</p><pre><code>python --version\n</code></pre><p>On Linux, you may use:</p><pre><code>python3 --version\n</code></pre><h2>5. ffmpeg (required)</h2><p>Official download page:</p><ul><li>https://ffmpeg.org/download.html</li></ul><p>OpenTutor uses <code>ffmpeg</code> for:</p><ul><li>Audio decoding compatibility in STT (Whisper dependency chain)</li><li>WAV -&gt; OGG/Opus conversion for WhatsApp voice notes</li></ul><h3>Option A (global): install in PATH</h3><p>Verify:</p><pre><code>ffmpeg -version\n</code></pre><h3>Option B (recommended for easier onboarding): local project binary</h3><p>Use this folder (setup scripts create it):</p><ul><li><code>tools/ffmpeg/</code></li></ul><p>Expected filenames:</p><ul><li>Windows: <code>tools/ffmpeg/ffmpeg.exe</code></li><li>Linux: <code>tools/ffmpeg/ffmpeg</code></li></ul><p>OpenTutor auto-detects this location before checking <code>PATH</code>.</p><p>On Linux:</p><pre><code>chmod +x ./tools/ffmpeg/ffmpeg\n</code></pre><h2>6. Ollama (local LLM)</h2><p>Official links:</p><ul><li>Download / website: https://ollama.com/</li><li>Model library: https://ollama.com/library</li></ul><p>Verify:</p><pre><code>ollama --version\n</code></pre><p>Pull a model (example):</p><pre><code>ollama pull llama3.1\n</code></pre><p>Other compatible examples:</p><ul><li><code>qwen2.5</code></li><li><code>mistral</code></li></ul><h3>Recommended models for weaker machines (CPU / lower RAM)</h3><p>If your machine is limited, start with smaller models (faster and lighter), for example:</p><ul><li><code>qwen2.5:3b</code> (good balance for a local tutor)</li><li><code>llama3.2:3b</code> (good lightweight option, if available in your Ollama setup)</li><li>quantized <code>7b</code> variants can work, but are usually much slower on CPU</li></ul><p>Practical guidance:</p><ul><li>Start with <code>3b</code></li><li>Move to <code>7b+</code> only if latency is acceptable</li></ul><p>Example:</p><pre><code>ollama pull qwen2.5:3b\n</code></pre><p>In <code>.env</code>:</p><pre><code>MODEL=qwen2.5:3b\n</code></pre><p>Quick test:</p><pre><code>ollama run llama3.1\n</code></pre><h2>7. Piper (local TTS)</h2><p>Official/reference links:</p><ul><li>Piper project (GitHub): https://github.com/rhasspy/piper</li><li>Piper voices (Hugging Face): https://huggingface.co/rhasspy/piper-voices</li></ul><p>Install the <code>piper</code> binary and verify:</p><pre><code>piper --help\n</code></pre><p>If you do not want it in <code>PATH</code>, set <code>PIPER_BIN</code> in <code>.env</code>.</p><h2>8. Download a Piper voice</h2><p>The project already includes helper scripts for the default voice:</p><p>Windows:</p><pre><code>.\\scripts\\download-piper-voice.ps1\n</code></pre><p>Linux:</p><pre><code>./scripts/download-piper-voice.sh\n</code></pre><p>Expected files (default example):</p><ul><li><code>services/tts/voices/en_US-lessac-medium.onnx</code></li><li><code>services/tts/voices/en_US-lessac-medium.onnx.json</code></li></ul><h2>9. Clone the project and install Node dependencies</h2><pre><code>git clone &lt;REPO_URL&gt;\ncd OpenTutor\npnpm install\n</code></pre><p>If <code>pnpm</code> is not available, the project can also run with <code>npm</code>, but <code>pnpm</code> is the recommended flow.</p><p>Important:</p><ul><li>Do a normal install (do not use <code>--ignore-scripts</code>), because <code>better-sqlite3</code> needs native bindings.</li><li>If you get a SQLite binding error on first startup, run:</li></ul><pre><code>npm rebuild better-sqlite3\n</code></pre><h2>10. Create and configure <code>.env</code></h2><p>Create it from the example:</p><p>Windows (PowerShell):</p><pre><code>Copy-Item .env.example .env\n</code></pre><p>Linux/macOS:</p><pre><code>cp .env.example .env\n</code></pre><p>Main fields to review:</p><ul><li><code>MODEL</code> (example: <code>llama3.1</code>)</li><li><code>OLLAMA_BASE_URL</code> (default <code>http://127.0.0.1:11434</code>)</li><li><code>STT_BASE_URL</code> (default <code>http://127.0.0.1:8001</code>)</li><li><code>PIPER_BIN</code> (if <code>piper</code> is not in PATH)</li><li><code>PIPER_MODEL</code></li><li><code>PIPER_CONFIG</code></li><li><code>FFMPEG_BIN</code> (optional if using <code>tools/ffmpeg</code> auto-detection)</li></ul><p>Example (Windows with local ffmpeg):</p><pre><code>MODEL=llama3.1\nFFMPEG_BIN=./tools/ffmpeg/ffmpeg.exe\nPIPER_MODEL=./services/tts/voices/en_US-lessac-medium.onnx\nPIPER_CONFIG=./services/tts/voices/en_US-lessac-medium.onnx.json\n</code></pre><h2>11. Configure the STT service (Python)</h2><p>Create a virtual environment inside <code>services/stt</code> and install dependencies.</p><h3>Windows (PowerShell)</h3><pre><code>cd services\\stt\npython -m venv .venv\n. .venv\\Scripts\\Activate.ps1\npip install -r requirements.txt\ncd ..\\..\n</code></pre><h3>Linux</h3><pre><code>cd services/stt\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\ncd ../..\n</code></pre><p>Optional performance tuning:</p><ul><li><code>STT_MODEL_SIZE=small</code> (good CPU default)</li><li><code>STT_MODEL_SIZE=tiny</code> (lighter for weak machines, lower accuracy)</li><li><code>STT_DEVICE=cpu</code></li><li><code>STT_COMPUTE_TYPE=int8</code></li></ul><p>Useful notes:</p><ul><li>On first startup, <code>faster-whisper</code> may download/prepare the model and take a bit longer.</li><li>You can test the STT service independently:</li></ul><pre><code>cd services/stt\npython -m uvicorn app:app --host 0.0.0.0 --port 8001\n</code></pre><p>In another terminal:</p><pre><code>curl http://127.0.0.1:8001/health\n</code></pre><p>On Windows (PowerShell):</p><pre><code>Invoke-WebRequest http://127.0.0.1:8001/health | Select-Object -ExpandProperty Content\n</code></pre><h2>12. Install Puppeteer Chrome (required by <code>whatsapp-web.js</code>)</h2><p>If the bot fails with a Chrome/Chromium not found error, install Puppeteer's managed Chrome:</p><pre><code>npx puppeteer browsers install chrome\n</code></pre><p>This usually fixes errors such as:</p><ul><li><code>Could not find Chrome (...)</code></li></ul><h2>13. Start everything and connect WhatsApp</h2><p>Run:</p><pre><code>pnpm dev\n</code></pre><p>This starts:</p><ul><li>STT service (FastAPI)</li><li>WhatsApp bot (Node.js)</li></ul><p>First run:</p><ul><li>A QR code appears in the terminal</li><li>Open WhatsApp on your phone</li><li>Go to <code>Linked Devices</code></li><li>Scan the QR code</li></ul><h2>14. Configure the tutor (initial commands)</h2><p>Send these commands to the bot:</p><pre><code>/start\n/language en\n/mode lesson\n/level A1\n/corrections light\n/voice on\n</code></pre><h2>15. Verification checklist (text + voice)</h2><p>Before testing, confirm:</p><ul><li><code>ollama</code> is running</li><li>the model is installed (<code>ollama list</code>)</li><li><code>ffmpeg -version</code> works (or a local binary exists in <code>tools/ffmpeg</code>)</li><li><code>piper --help</code> works (or <code>PIPER_BIN</code> is configured)</li><li>Piper voice files exist in <code>services/tts/voices/</code></li><li>STT dependencies are installed in <code>.venv</code></li><li>Puppeteer Chrome is installed (<code>npx puppeteer browsers install chrome</code>)</li></ul><p>Test:</p><ul><li>Send a text message in WhatsApp</li><li>Send a voice note in WhatsApp</li><li>Confirm the bot replies with text and/or voice</li></ul><h2>16. If something fails</h2><p>Check:</p><ul><li><code>docs/TROUBLESHOOTING.md</code></li><li><code>docs/QUICKSTART.md</code></li></ul><h2>17. Recommended full setup flow (exact commands from scratch)</h2><h3>Windows (PowerShell)</h3><pre><code>git clone &lt;REPO_URL&gt;\ncd OpenTutor\nnpm install -g pnpm\npnpm install\nCopy-Item .env.example .env\nnpx puppeteer browsers install chrome\n\ncd services\\stt\npython -m venv .venv\n. .venv\\Scripts\\Activate.ps1\npip install -r requirements.txt\ncd ..\\..\n\n# Install Ollama, ffmpeg, Piper, and a Piper voice manually\n# Optional: put ffmpeg at .\\tools\\ffmpeg\\ffmpeg.exe\n\npnpm dev\n</code></pre><h3>Linux</h3><pre><code>git clone &lt;REPO_URL&gt;\ncd OpenTutor\nnpm install -g pnpm\npnpm install\ncp .env.example .env\nnpx puppeteer browsers install chrome\n\ncd services/stt\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\ncd ../..\n\n# Install Ollama, ffmpeg, Piper, and a Piper voice manually\n# Optional: put ffmpeg at ./tools/ffmpeg/ffmpeg\n\npnpm dev\n</code></pre><h2>Quick summary (recommended order)</h2><ul><li>Install Git, Node.js, pnpm, Python</li><li>Install <code>ffmpeg</code></li><li>Install Ollama and pull a model</li><li>Install Piper and download a voice</li><li>Run <code>pnpm install</code></li><li>Create <code>.env</code></li><li>Install <code>services/stt</code> dependencies (<code>pip install -r requirements.txt</code>)</li><li>Run <code>npx puppeteer browsers install chrome</code></li><li>Run <code>pnpm dev</code></li><li>Scan the QR code</li></ul>"},{"id":"QUICKSTART.md","title":"QUICKSTART","html":"<h1>Quickstart</h1><p>This guide gets OpenTutor running locally with WhatsApp text + voice.</p><h2>1. Requirements</h2><ul><li>Node.js <code>20+</code></li><li><code>pnpm</code> (recommended)</li><li>Python <code>3.10+</code></li><li><code>ffmpeg</code> (either in <code>PATH</code> or placed inside <code>./tools/ffmpeg/</code>)</li><li>Ollama installed</li><li>Piper binary installed</li></ul><h2>2. Clone and install</h2><pre><code>git clone &lt;your-fork-or-repo-url&gt;\ncd OpenTutor\npnpm install\n</code></pre><p>Important:</p><ul><li>Do not use <code>--ignore-scripts</code> on install, because <code>better-sqlite3</code> needs native bindings.</li><li>If you see a <code>better_sqlite3.node</code> binding error, run:</li></ul><pre><code>npm rebuild better-sqlite3\n</code></pre><h2>3. Configure environment</h2><pre><code>cp .env.example .env\n</code></pre><p>On Windows PowerShell:</p><pre><code>Copy-Item .env.example .env\n</code></pre><p>Important variables in <code>.env</code>:</p><ul><li><code>MODEL</code> (Ollama model name)</li><li><code>OLLAMA_BASE_URL</code></li><li><code>STT_BASE_URL</code></li><li><code>PIPER_BIN</code></li><li><code>PIPER_MODEL</code></li><li><code>PIPER_CONFIG</code></li><li><code>FFMPEG_BIN</code></li></ul><h2>4. Install Ollama and model</h2><p>Install Ollama from the official website for your OS, then pull a model:</p><pre><code>ollama pull llama3.1\n</code></pre><p>You can use other installed models too (examples):</p><ul><li><code>qwen2.5</code></li><li><code>mistral</code></li><li><code>phi4</code></li></ul><p>Set the one you want in <code>.env</code>:</p><pre><code>MODEL=llama3.1\n</code></pre><h2>5. Install ffmpeg</h2><p>OpenTutor needs <code>ffmpeg</code> for:</p><ul><li>STT decoding compatibility (Faster-Whisper dependency)</li><li>WAV -&gt; OGG/Opus conversion for WhatsApp voice notes</li></ul><p>OpenTutor supports two ways to use <code>ffmpeg</code>:</p><ul><li>Global install in <code>PATH</code></li><li>Local project binary in <code>./tools/ffmpeg/</code> (recommended for easier setup sharing)</li></ul><p>If <code>FFMPEG_BIN</code> is not set, OpenTutor tries:</p><ul><li>Windows: <code>./tools/ffmpeg/ffmpeg.exe</code></li><li>Linux: <code>./tools/ffmpeg/ffmpeg</code></li><li>then falls back to <code>ffmpeg</code> from <code>PATH</code></li></ul><h3>Windows</h3><ul><li>Install via <code>winget</code>, <code>choco</code>, or manual zip</li><li>Ensure <code>ffmpeg.exe</code> is in <code>PATH</code>, or copy it to <code>.\\tools\\ffmpeg\\ffmpeg.exe</code></li></ul><p>Example (<code>winget</code>, may vary by package id):</p><pre><code>winget install ffmpeg\n</code></pre><h3>Linux (Debian/Ubuntu)</h3><pre><code>sudo apt-get update\nsudo apt-get install -y ffmpeg\n</code></pre><p>If you want a project-local binary instead of PATH, copy it to:</p><pre><code>./tools/ffmpeg/ffmpeg\nchmod +x ./tools/ffmpeg/ffmpeg\n</code></pre><h2>6. Install Piper and voice model</h2><p>Install the <code>piper</code> binary (OS package / release binary), then download a voice.</p><h3>Download default voice</h3><p>Windows:</p><pre><code>.\\scripts\\download-piper-voice.ps1\n</code></pre><p>Linux:</p><pre><code>./scripts/download-piper-voice.sh\n</code></pre><p>This places files under <code>services/tts/voices/</code>.</p><p>If <code>piper</code> is not in <code>PATH</code>, update <code>.env</code>:</p><pre><code>PIPER_BIN=C:\\path\\to\\piper.exe\n</code></pre><h2>7. Set up STT Python service</h2><h3>Windows (PowerShell)</h3><pre><code>cd services\\stt\npython -m venv .venv\n. .venv\\Scripts\\Activate.ps1\npip install -r requirements.txt\ncd ..\\..\n</code></pre><h3>Linux</h3><pre><code>cd services/stt\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\ncd ../..\n</code></pre><p>Optional STT tuning:</p><pre><code>export STT_MODEL_SIZE=small\nexport STT_DEVICE=cpu\nexport STT_COMPUTE_TYPE=int8\n</code></pre><p>You can verify STT before running the bot:</p><pre><code>cd services/stt\npython -m uvicorn app:app --host 0.0.0.0 --port 8001\n</code></pre><p>Then check:</p><pre><code>curl http://127.0.0.1:8001/health\n</code></pre><h2>7.5 Install Puppeteer Chrome (required by <code>whatsapp-web.js</code>)</h2><p>If Chrome/Chromium is not found on first run, install Puppeteer's managed Chrome:</p><pre><code>npx puppeteer browsers install chrome\n</code></pre><h2>8. Run OpenTutor (bot + STT)</h2><pre><code>pnpm dev\n</code></pre><p>This starts:</p><ul><li>STT FastAPI service on <code>http://127.0.0.1:8001</code></li><li>WhatsApp bot app</li></ul><p>On first run, a QR code appears in the terminal.</p><h2>9. WhatsApp login</h2><ul><li>Open WhatsApp on your phone</li><li>Linked Devices</li><li>Scan the QR shown in the terminal</li><li>Wait for <code>client ready</code></li></ul><p>Sessions are stored locally in <code>./sessions</code> (configurable).</p><h2>10. First commands</h2><p>Send these to the bot in WhatsApp:</p><pre><code>/start\n/language en\n/mode lesson\n/level A1\n/corrections light\n/voice on\n</code></pre><p>Then send text or a voice note.</p><h2>11. Verify voice pipeline</h2><p>Send a short voice note in the target language.</p><p>Expected flow:</p><ul><li>Bot transcribes audio (STT)</li><li>Bot generates tutor reply (Ollama)</li><li>Bot returns text + voice note (Piper + ffmpeg)</li></ul><p>If it fails, see <code>TROUBLESHOOTING.md</code>.</p>"},{"id":"ARCHITECTURE.md","title":"ARCHITECTURE","html":"<h1>Architecture</h1><h2>Overview</h2><p>OpenTutor is a local-first, modular system:</p><ul><li>WhatsApp interface in Node.js/TypeScript</li><li>Tutor Engine in TypeScript (FSM + prompts + memory policy)</li><li>SQLite persistence in TypeScript</li><li>Local LLM via Ollama HTTP API</li><li>Local STT via FastAPI + Faster-Whisper</li><li>Local TTS via Piper CLI + ffmpeg</li></ul><h2>High-level Data Flow</h2><pre><code>WhatsApp User\n   |\n   v\nwhatsapp-web.js (apps/whatsapp)\n   |-- text ------------------------------+\n   |-- audio -&gt; media download -&gt; STT ----+--&gt; Tutor Engine (core/tutor)\n                                           |    |-- load profile/settings/state/summary/history (core/db)\n                                           |    |-- FSM transition + policy selection\n                                           |    |-- prompt assembly\n                                           |    '-- Ollama generate (core/llm)\n                                           |\n                                           +--&gt; reply text\n                                                |-- save messages/logs to SQLite\n                                                |-- optional summary refresh every N messages\n                                                '-- TTS (Piper -&gt; WAV, ffmpeg -&gt; OGG/Opus)\n                                                          |\n                                                          v\n                                                    WhatsApp reply (text + voice)\n</code></pre><h2>Module Responsibilities</h2><h3><code>apps/whatsapp</code></h3><ul><li>WhatsApp client lifecycle (<code>LocalAuth</code>, QR, ready/disconnect)</li><li>Message routing (text vs audio)</li><li>Slash command parsing and settings updates</li><li>Rate limiting / loop guard</li><li>STT and TTS integration</li><li>Error handling and user-facing fallback messages</li></ul><h3><code>core/tutor</code></h3><ul><li>Finite state machine (FSM): <code>IDLE</code>, <code>LESSON_INTRO</code>, <code>PRACTICE</code>, <code>FEEDBACK</code></li><li>Pedagogical policies (corrections, response length, question count)</li><li>Prompt construction using deterministic context inputs</li><li>Memory summary refresh policy (every N messages)</li></ul><h3><code>core/db</code></h3><ul><li>SQLite schema + migrations</li><li>Persistent user profile, settings, session state</li><li>Message history and memory summary</li><li>Vocabulary and mistake logging</li><li>Event logs for debugging and operations</li></ul><h3><code>core/llm</code></h3><ul><li>Provider abstraction (<code>LlmProvider</code>)</li><li>Ollama client (<code>POST /api/generate</code>, <code>stream=false</code>)</li><li>Request timeout handling</li></ul><h3><code>services/stt</code></h3><ul><li>FastAPI HTTP endpoint <code>/transcribe</code></li><li>Local <code>faster-whisper</code> inference</li><li>Optional language hint</li></ul><h2>Why the LLM does not control everything</h2><p>The Tutor Engine enforces:</p><ul><li>Current mode (<code>chat</code>, <code>lesson</code>, <code>drill</code>, <code>exam</code>)</li><li>Correction policy (<code>off</code>, <code>light</code>, <code>strict</code>)</li><li>FSM state and task</li><li>Max response length</li><li>Memory summary cadence</li><li>Prompt inputs (profile + summary + recent messages + task)</li></ul><p>This keeps tutor behavior more stable and product-like than raw chat completion.</p><h2>Persistence Model (SQLite)</h2><p>Main tables:</p><ul><li><code>users</code></li><li><code>user_settings</code></li><li><code>session_state</code></li><li><code>user_memory</code></li><li><code>messages</code></li><li><code>vocab_seen</code></li><li><code>mistakes</code></li><li><code>event_logs</code></li></ul><p>Database path defaults to:</p><ul><li><code>./data/opentutor.sqlite</code></li></ul><h2>Cross-platform notes</h2><ul><li>Node app works on Windows/Linux</li><li>STT service works on Windows/Linux with Python + ffmpeg</li><li>TTS works on Windows/Linux if <code>piper</code> and <code>ffmpeg</code> are installed</li><li><code>scripts/</code> contains platform-specific helpers</li></ul>"},{"id":"PEDAGOGY.md","title":"PEDAGOGY","html":"<h1>Pedagogy</h1><p>OpenTutor is designed as a language tutor, not a generic chatbot.</p><h2>Core principles</h2><ul><li>Short replies</li><li>One question at a time</li><li>Active production over passive explanation</li><li>Limited corrections to avoid overload</li><li>Repetition and recall</li><li>Context continuity via memory summary</li></ul><h2>Tutor Engine rules</h2><p>The Tutor Engine builds every LLM prompt from:</p><ul><li>Learner profile (level, goal)</li><li>Learner settings (mode, target language, correction mode, voice preference)</li><li>Session FSM state + current task</li><li>Memory summary (persisted)</li><li>Recent message history</li><li>Current learner input</li></ul><p>The LLM does not choose the teaching strategy alone.</p><h2>Modes</h2><h3><code>chat</code></h3><ul><li>Free conversation in target language</li><li>Minimal structure, still concise</li><li>One follow-up question</li></ul><h3><code>lesson</code></h3><ul><li>Micro-topic introduction</li><li>Guided examples</li><li>Short practice prompts</li></ul><h3><code>drill</code></h3><ul><li>Repetition and pattern practice</li><li>Fast prompts</li><li>Quick correction cycles</li></ul><h3><code>exam</code></h3><ul><li>More strict evaluation</li><li>Fewer hints</li><li>Avoid revealing answers too early</li></ul><h2>Correction policies</h2><h3><code>off</code></h3><ul><li>No explicit corrections unless learner asks</li></ul><h3><code>light</code></h3><ul><li>Correct only high-impact errors</li><li>Max ~2 corrections</li><li>Keep flow moving</li></ul><h3><code>strict</code></h3><ul><li>More direct correction</li><li>Max ~3 corrections</li><li>Still concise (no long grammar essay)</li></ul><h2>FSM (Finite State Machine)</h2><p>States:</p><ul><li><code>IDLE</code></li><li><code>LESSON_INTRO</code></li><li><code>PRACTICE</code></li><li><code>FEEDBACK</code></li></ul><p>Why use FSM:</p><ul><li>Prevents chaotic tutor behavior</li><li>Makes mode behavior more predictable</li><li>Gives the model a concrete current task</li></ul><h2>Memory summary</h2><p>Every N messages (default <code>8</code>), OpenTutor asks the LLM to generate a compact summary of:</p><ul><li>Level signals</li><li>Recurring mistakes</li><li>Useful vocabulary</li><li>Goal</li><li>Recommended next practice</li></ul><p>This summary is stored in SQLite and injected into future prompts.</p><h2>Why this works better than raw chat</h2><ul><li>The tutor remains focused on learning outcomes</li><li>Corrections are bounded</li><li>Prompt context is compact and persistent</li><li>User settings directly shape behavior</li></ul>"},{"id":"COMMANDS.md","title":"COMMANDS","html":"<h1>Commands</h1><p>OpenTutor supports the following WhatsApp commands.</p><h2>Onboarding and help</h2><ul><li><code>/start</code> : show onboarding and recommended setup</li><li><code>/help</code> : show command list</li></ul><h2>Mode and learning behavior</h2><ul><li><code>/mode chat|lesson|drill|exam</code></li><li><code>/level &lt;value&gt;</code></li><li><code>/goal &lt;free text&gt;</code></li><li><code>/corrections off|light|strict</code></li><li><code>/language &lt;target language code&gt;</code></li></ul><h2>Voice behavior</h2><ul><li><code>/voice on|off</code></li></ul><h2>Additional utility</h2><ul><li><code>/settings</code> : print current settings summary</li></ul><h2>Examples</h2><pre><code>/start\n/language en\n/mode lesson\n/level A2\n/goal Speak confidently in work meetings\n/corrections light\n/voice on\n</code></pre><h2>Notes</h2><ul><li>Commands are parsed from text messages starting with <code>/</code></li><li>Unknown commands return help text</li><li>Commands are persisted per user in SQLite</li></ul>"},{"id":"TROUBLESHOOTING.md","title":"TROUBLESHOOTING","html":"<h1>Troubleshooting</h1><h2>QR code does not appear</h2><ul><li>Check that the bot process is running (<code>pnpm dev</code>)</li><li>Ensure terminal supports text output (QR prints in terminal)</li><li>If session data is corrupted, stop the app and remove <code>./sessions</code> (you will need to re-login)</li></ul><h2>WhatsApp client fails to start (Puppeteer / browser issues)</h2><ul><li>Install Puppeteer's managed Chrome:</li></ul><pre><code>npx puppeteer browsers install chrome\n</code></pre><ul><li>Install system dependencies required by Chromium (Linux)</li><li>Try updating <code>whatsapp-web.js</code></li><li>Ensure no security software is blocking local browser launch</li></ul><p>Typical error:</p><ul><li><code>Could not find Chrome (...)</code></li></ul><h2><code>better-sqlite3</code> binding missing (<code>better_sqlite3.node</code>)</h2><p>If startup fails with a native binding error, the SQLite package was likely installed without running scripts or needs a rebuild.</p><p>Fix:</p><pre><code>npm rebuild better-sqlite3\n</code></pre><p>Or reinstall normally (without <code>--ignore-scripts</code>):</p><pre><code>pnpm install\n</code></pre><h2>Bot replies to no messages</h2><ul><li>Confirm <code>client ready</code> appears in logs</li><li>Verify you are messaging the linked account (not a group if <code>ALLOW_GROUPS=false</code>)</li><li>Check SQLite logs in <code>event_logs</code> table</li><li>Verify Ollama is reachable (<code>OLLAMA_BASE_URL</code>)</li></ul><h2>Ollama errors / timeouts</h2><ul><li>Start Ollama locally</li><li>Check model is installed:</li></ul><pre><code>ollama list\n</code></pre><ul><li>Pull a model if missing:</li></ul><pre><code>ollama pull llama3.1\n</code></pre><ul><li>Increase <code>OLLAMA_TIMEOUT_MS</code> for slower CPUs/models</li></ul><h2>STT transcribe fails</h2><ul><li>Ensure STT service is running on <code>http://127.0.0.1:8001</code></li><li>Check <code>services/stt/.venv</code> is installed correctly</li><li>Confirm <code>ffmpeg</code> is in <code>PATH</code></li><li>Try a smaller STT model (<code>STT_MODEL_SIZE=tiny</code> or <code>small</code>)</li><li>Inspect STT logs in terminal</li><li>Verify <code>/health</code> responds:</li></ul><pre><code>curl http://127.0.0.1:8001/health\n</code></pre><p>Common first-run issues:</p><ul><li><code>No module named uvicorn</code> -&gt; run <code>pip install -r requirements.txt</code> in <code>services/stt/.venv</code></li><li><code>No module named requests</code> -&gt; update/install dependencies again (<code>pip install -r requirements.txt</code>)</li></ul><h2>Audio received but no voice reply</h2><ul><li>Check <code>piper</code> binary path (<code>PIPER_BIN</code>)</li><li>Verify voice files exist:</li><li><code>services/tts/voices/*.onnx</code></li><li><code>services/tts/voices/*.onnx.json</code></li><li>Check <code>ffmpeg</code> conversion works (<code>ffmpeg -version</code>)</li><li>If using project-local ffmpeg, verify file exists:</li><li>Windows: <code>tools/ffmpeg/ffmpeg.exe</code></li><li>Linux: <code>tools/ffmpeg/ffmpeg</code></li><li>Try <code>/voice off</code> to validate text path independently</li></ul><h2>Piper exits with error</h2><ul><li>Wrong model/config path in <code>.env</code></li><li>Incompatible voice files</li><li>Binary not executable (Linux permissions)</li></ul><h2>WhatsApp voice format issues</h2><ul><li>OpenTutor converts output to <code>OGG/Opus</code> using <code>ffmpeg</code></li><li>Verify <code>libopus</code> support in your ffmpeg build</li></ul><h2>Group messages are ignored</h2><p>Default behavior is to ignore groups.</p><p>Set in <code>.env</code>:</p><pre><code>ALLOW_GROUPS=true\n</code></pre><h2>Rate limiting blocks messages</h2><p>Adjust in <code>.env</code>:</p><ul><li><code>RATE_LIMIT_WINDOW_MS</code></li><li><code>RATE_LIMIT_MAX_MESSAGES</code></li><li><code>MIN_SECONDS_BETWEEN_REPLIES</code></li></ul>"},{"id":"SECURITY.md","title":"SECURITY","html":"<h1>Security</h1><h2>Important warning</h2><p>OpenTutor uses <code>whatsapp-web.js</code>, which is not an official WhatsApp Business API integration.</p><p>Use at your own risk and review platform terms for your use case.</p><h2>Local data storage</h2><p>OpenTutor stores data locally in SQLite:</p><ul><li>User settings</li><li>Learning progress summary</li><li>Message history excerpts</li><li>Mistakes/vocabulary</li><li>Event logs</li></ul><p>Database default path:</p><ul><li><code>./data/opentutor.sqlite</code></li></ul><h2>Secrets</h2><ul><li>Do not commit <code>.env</code></li><li>Do not commit session files (<code>./sessions</code>)</li><li>Do not commit local databases or logs</li></ul><h2>Operational safeguards</h2><ul><li>Per-user rate limiting</li><li>Minimum reply spacing</li><li>Loop protection for repeated responses</li><li>Response length caps</li><li>Groups ignored by default (configurable)</li></ul><h2>Privacy notes</h2><ul><li>STT, LLM, and TTS are local services when configured as documented</li><li>No paid/cloud API keys are required</li><li>Privacy still depends on your machine security and local network settings</li></ul><h2>Hardening suggestions</h2><ul><li>Restrict local service ports to localhost</li><li>Use OS-level disk encryption</li><li>Run under a non-admin user</li><li>Back up SQLite securely</li><li>Rotate/delete message history if needed for compliance</li></ul>"},{"id":"LICENSE_NOTES.md","title":"LICENSE_NOTES","html":"<h1>License Notes</h1><p>OpenTutor repository code is MIT licensed (<code>LICENSE</code>).</p><h2>Third-party runtime components (installed separately)</h2><ul><li>**Ollama**: separate project, separate license</li><li>**whatsapp-web.js**: separate project, separate license</li><li>**Faster-Whisper / CTranslate2**: separate licenses</li><li>**FastAPI / Uvicorn**: separate licenses</li><li>**Piper**: separate project, separate license</li><li>**ffmpeg**: separate project, separate license</li></ul><h2>Models and voices</h2><p>Language models (Ollama) and Piper voice models are not included in this repository.</p><p>Their licenses vary by model/voice. Verify licenses before production or commercial use.</p><h2>WhatsApp platform use</h2><p>Using <code>whatsapp-web.js</code> may have platform policy implications. Evaluate your risk and compliance requirements.</p>"}];

const docListEl = document.getElementById("doc-list");
const titleEl = document.getElementById("doc-title");
const rootEl = document.getElementById("markdown-root");

function getDoc(id){ return DOCS.find((d)=>d.id===id) || DOCS[0]; }
function loadDoc(id){ const doc=getDoc(id); titleEl.textContent=doc.title; rootEl.innerHTML=doc.html; }
function buildNav(){ const currentId=new URLSearchParams(window.location.search).get("doc") || DOCS[0].id; for (const doc of DOCS){ const btn=document.createElement("button"); btn.textContent=doc.title; if(doc.id===currentId) btn.classList.add("active"); btn.addEventListener("click", ()=>{ const url=new URL(window.location.href); url.searchParams.set("doc", doc.id); window.history.pushState({},"",url); [...docListEl.querySelectorAll("button")].forEach((b)=>b.classList.remove("active")); btn.classList.add("active"); loadDoc(doc.id); }); docListEl.appendChild(btn);} loadDoc(currentId);}
window.addEventListener("popstate", ()=>{ const currentId=new URLSearchParams(window.location.search).get("doc") || DOCS[0].id; [...docListEl.querySelectorAll("button")].forEach((btn)=>{ btn.classList.toggle("active", btn.textContent===getDoc(currentId).title); }); loadDoc(currentId);});
buildNav();
